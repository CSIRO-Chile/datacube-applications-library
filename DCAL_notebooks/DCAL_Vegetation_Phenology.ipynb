{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Landsat Vegetation Phenology <img align=\"right\" style=\"padding:10px\" src=\"../images/odc_logo.png\">\n",
    "\n",
    "This notebook calculates vegetation phenology changes using Landsat 7 or Landsat 8 data. To detect changes, the algorithm uses Normalized Difference Vegetation Index (NDVI) which is a common proxy for vegetation growth and health. The outputs of this notebook can be used to assess differences in agriculture fields over time or space and also allow the assessment of growing states such as planting and harvesting.  \n",
    "<br>\n",
    "There are two output products. The first output product is a time series boxplot of NDVI with the data binned by week, month, week of year, or month of year. The second output product is a time series lineplot of the mean NDVI for each year, with the data potentially binned by week or month. This product is useful for comparing years to each other.\n",
    "<br><br>\n",
    "Finally, the annual NDVI curves are analyzed using TIMESAT to calculate the seasonality parameters: (a) beginning of season, (b) end of season, (c) middle of season, (d) length of season, (e) base value, (f) maximum value, (g) amplitude. These parameters are often used by agriculture scientists.\n",
    "\n",
    "What does this Notebook do?\n",
    "* [Setup the Environment](#environment_setup)\n",
    "* [Choose Platforms and Products](#platforms_products)\n",
    "* [Define Anlaysis Parameters](#define_analysis_params)\n",
    "* [Load the Dataset and the Required Spectral Bands or Other Parameters](#load_data)\n",
    "* [Create a Mosaic for the Baseline and Analysis Time Periods](#create_mosaic)\n",
    "* [Calculate Anomaly Product](#anomalies)\n",
    "* [Show Baseline RGB, Analysis RGB and Anomaly Products](#plots)\n",
    "* [Calculate Pixel Counts within Thresholds](#calculate_pixels)\n",
    "\n",
    "<i><p style=\"color:red\"><b>Red Text:</b> Indicates code blocks where user inputs can be provided.</p></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"environment_setup\"></a>\n",
    "## Environment Setup [&#9652;](#top)\n",
    "* **Surpress Python Warnings:** These warnings include notes about deprecitation of Python modules, etc.\n",
    "* **Import Standard Python Modules:** Import external and standard Python modules including `datacube` which is part of the ODC framework, as well as `xarray` which is a useful external module.\n",
    "* **Import DCAL Utilities:** Import a set of utilities developed by the DCAL team to help automate some of the more routine tasks in the notebooks. These utilities are located in `/DCAL_utils`.\n",
    "* **Initialize Data Cube and Data Cube API**\n",
    "\n",
    "[Back to Top](#top)## <span id=\"import\">Import Dependencies and Connect to the Data Cube [&#9652;](#top)</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Standard Python Modules\n",
    "import datacube\n",
    "import numpy as np  \n",
    "import xarray as xr  \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add utils to the sys.path so that data_cube_utilities can be found. (Local folder hack.)\n",
    "import sys\n",
    "sys.path.append('../DCAL_utils')\n",
    "\n",
    "# Import Data Cube API.\n",
    "import data_access_api as dc_api\n",
    "\n",
    "# Import overlapping area function\n",
    "from dc_load import get_overlapping_area\n",
    "\n",
    "# Import date time function\n",
    "from dc_time import dt_to_str\n",
    "\n",
    "# Import display map functions\n",
    "from dc_display_map import display_map\n",
    "\n",
    "# Special functions used for vegetation proxy calculations...\n",
    "# GD - STOPPED HERE - APPERS THAT utils_special IS A BROAD REWORK OF THE utils FOLDER\n",
    "# HOW TO RECONCILE, OR MAINTAIN PARALLEL utils AND utils_special NEEDS TO BE DISCUSSED/DECIDED\n",
    "#from utils_special.data_cube_utilities.dc_load import match_dim_sizes\n",
    "#from utils.data_cube_utilities.clean_mask import \\\n",
    "#    landsat_qa_clean_mask, landsat_clean_mask_invalid\n",
    "#from utils.data_cube_utilities.sort import xarray_sortby_coord\n",
    "#from utils_special.data_cube_utilities.dc_load import is_dataset_empty\n",
    "#from utils_special.data_cube_utilities.aggregate import xr_scale_res\n",
    "\n",
    "# Import match dimensions function\n",
    "#from dc_load import match_dim_sizes\n",
    "\n",
    "# Import clean mask functions\n",
    "from clean_mask import landsat_qa_clean_mask, landsat_clean_mask_invalid\n",
    "\n",
    "# Import xarray sort function\n",
    "#from sort import xarray_sortby_coord\n",
    "\n",
    "# Import dataset empty test function\n",
    "#from dc_load import is_dataset_empty\n",
    "\n",
    "# Import scaling function\n",
    "#from aggregate import xr_scale_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Data Cube (dc) and Data Cube API (api).\n",
    "api = dc_api.DataAccessApi()\n",
    "dc = api.dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"platforms_products\"></a>\n",
    "## <span id=\"plat_prod\">Choose Platforms and Products [&#9652;](#top)</span>\n",
    "* **Select Product:** Specify the product you desire to use. Note you can see the complete list of products in this Cube in the introductory notebook (<a href=\"../Open%20Data%20Cube%20Application%20Library%20Notebooks.ipynb#products\" target=\"_blank\">click here</a>).\n",
    "* **View Dataset Spatial and Temporal Extents:** Queries the Cube and displays the min/max latitude and longitude, and the start and end date and time.\n",
    "* **Select Spatial and Temporal Extents:** Select the area of interest (AOI) and time range based on extents available.\n",
    "\n",
    "[Back to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Products and Platforms\n",
    "# It is possible to select multiple datasets (L7, L8)\n",
    "# True = SELECT\n",
    "# False = DO NOT SELECT\n",
    "# Possible cubes are: kenya, ghana, senegal, tanzania, sierra_leone\n",
    "\n",
    "use_Landsat7 = True\n",
    "use_Landsat8 = True\n",
    "platforms, products = [], []\n",
    "if use_Landsat7:\n",
    "    platforms.append('LANDSAT_7')\n",
    "    products.append('ls7_ledaps_ghana')\n",
    "if use_Landsat8:\n",
    "    platforms.append('LANDSAT_8')\n",
    "    products.append('ls8_lasrc_ghana')\n",
    "\n",
    "veg_proxy = 'NDVI' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For platform LANDSAT_7 and product ls7_ledaps_ghana:\n",
      "Time Extents: ['2000-01-01' '2020-01-10']\n",
      "\n",
      "For platform LANDSAT_8 and product ls8_lasrc_ghana:\n",
      "Time Extents: ['2013-04-13' '2020-01-27']\n",
      "\n",
      "Overlapping Extents:\n",
      "Latitude Extents: (3.772924193304, 11.318772579912)\n",
      "Longitude Extents: (-3.772924193304, 1.886462096652)\n",
      "Time Extents: ['2013-04-13', '2020-01-10']\n"
     ]
    }
   ],
   "source": [
    "full_lat, full_lon, min_max_dates = get_overlapping_area(api, platforms, products)\n",
    "\n",
    "# Print the extents of each product.\n",
    "str_min_max_dates = np.vectorize(dt_to_str)(min_max_dates)\n",
    "for i, (platform, product) in enumerate(zip(platforms, products)):\n",
    "    print(\"For platform {} and product {}:\".format(platform, product))\n",
    "    print(\"Time Extents:\", str_min_max_dates[i])\n",
    "    print()\n",
    "\n",
    "# Print the extents of the combined data.\n",
    "min_start_date_mutual = np.max(min_max_dates[:,0])\n",
    "max_end_date_mutual = np.min(min_max_dates[:,1])\n",
    "print(\"Overlapping Extents:\")\n",
    "print(\"Latitude Extents:\", full_lat)\n",
    "print(\"Longitude Extents:\", full_lon)\n",
    "print(\"Time Extents:\", list(map(dt_to_str, (min_start_date_mutual, max_end_date_mutual))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\";><b>Select Spatial Extents:</b> Select the area of interest (AOI) based on extents available.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an analysis region (Lat-Lon) within the extents listed above. \n",
    "# Select a time period (Min-Max) within the extents listed above (Year-Month-Day)\n",
    "\n",
    "# Time Period\n",
    "start_date, end_date = dt.datetime(2015,1,1), dt.datetime(2017,12,31)\n",
    "time_extents = (start_date, end_date)\n",
    "\n",
    "# Maize Field in Ghana\n",
    "# lat = (9.634628, 9.645304)\n",
    "# lon = (-0.957238, -0.940233)\n",
    "\n",
    "# Sudan Savanna, Ghana \n",
    "lat = (11.1023, 11.1265)\n",
    "lon = (-0.2487, -0.2213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><iframe src=\"data:text/html;charset=utf-8;base64,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\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x7f3e60044d30>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The code below renders a map that can be used to orient yourself with the region.\n",
    "display_map(lat, lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load_data\"></a>\n",
    "## Load Data from the Data Cube and Obtain the Vegetation Proxy [&#9652;](#top)\n",
    "* **Load Data:** Load data, clean, and combine.\n",
    "* **NDVI:** Compute NDVI.\n",
    "\n",
    "[Back to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = ['red', 'nir', 'pixel_qa']\n",
    "\n",
    "datasets, clean_masks = {}, {}\n",
    "matching_abs_res, same_dim_sizes = match_dim_sizes(dc, products, lon, lat)\n",
    "for platform, product in zip(platforms, products):\n",
    "    # Load the data.\n",
    "    dataset = dc.load(platform=platform, product=product, lat=lat, lon=lon, \n",
    "                      time=time_extents, measurements=measurements)\n",
    "    if len(dataset.dims) == 0: # The dataset is empty.\n",
    "        continue\n",
    "    # Get the clean masks.\n",
    "    clean_mask = (landsat_qa_clean_mask(dataset, platform) & \n",
    "                  (dataset[measurements[0]] != -9999) & \n",
    "                  landsat_clean_mask_invalid(dataset))\n",
    "    dataset = dataset.drop('pixel_qa')\n",
    "    # If needed, scale the datasets and clean masks to the same size in the x and y dimensions.\n",
    "    if not same_dim_sizes:    \n",
    "        dataset = xr_scale_res(dataset, abs_res=matching_abs_res)\n",
    "        clean_mask = xr_scale_res(clean_mask.astype(np.uint8), \\\n",
    "                                  abs_res=matching_abs_res).astype(np.bool)\n",
    "    dataset = dataset.astype(np.float32).where(clean_mask)\n",
    "    # Collect the dataset and clean mask.\n",
    "    datasets[platform] = dataset\n",
    "    clean_masks[platform] = clean_mask\n",
    "# Combine everything.\n",
    "if len(datasets) > 0:\n",
    "    dataset = xarray_sortby_coord(\n",
    "        xr.concat(list(datasets.values()), dim='time'), coord='time')\n",
    "    clean_mask = xarray_sortby_coord(\n",
    "        xr.concat(list(clean_masks.values()), dim='time'), coord='time')\n",
    "else:\n",
    "    dataset = xr.Dataset()\n",
    "    clean_mask = xr.DataArray(np.empty((0,), dtype=np.bool))\n",
    "del datasets, clean_masks\n",
    "\n",
    "assert not is_dataset_empty(dataset), \"The dataset is empty.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils_special.data_cube_utilities.vegetation import NDVI\n",
    "dataset[veg_proxy] = NDVI(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span id=\"phenology_plot_1\">Plot the Vegetation Index Over Time in a Box-and-Whisker Plot [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils_special.data_cube_utilities.plotter_utils import xarray_time_series_plot\n",
    "\n",
    "# Specify whether to plot a curve fit of the vegetation index along time. Input can be either TRUE or FALSE\n",
    "plot_curve_fit = True\n",
    "# assert isinstance(plot_curve_fit, bool), \"The variable 'plot_curve_fit' must be \"\\\n",
    "#                                         \"either True or False.\"\n",
    "\n",
    "# Specify the target aggregation type of the curve fit. Input can be either 'mean' or 'median'.\n",
    "curve_fit_target = 'median'\n",
    "# assert curve_fit_target in ['mean', 'median'], \"The variable 'curve_fit_target' must be either \"\\\n",
    "#                                               \"'mean' or 'median'.\"\n",
    "\n",
    "# The maximum number of data points that appear along time in each plot.\n",
    "# If more than this number of data points need to be plotted, a grid of plots will be created.\n",
    "max_times_per_plot = 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the binning approach for the vegetation index. \n",
    "#  None         = do not bin the data\n",
    "# 'week'        = bin the data by week with an extended time axis\n",
    "# 'month'       = bin the data by month with an extended time axis\n",
    "# 'weekofyear'  = bin the data by week and years using a single year time axis\n",
    "# 'monthofyear' = bin the data by month and years using a single year time axis\n",
    "\n",
    "bin_by = 'month'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "assert bin_by in [None, 'week', 'month', 'weekofyear', 'monthofyear'], \\\n",
    "    \"The variable 'bin_by' can only have one of these values: \"\\\n",
    "    \"[None, 'week', 'month', 'weekofyear', 'monthofyear']\"\n",
    "\n",
    "aggregated_by_str = None\n",
    "if bin_by is None:\n",
    "    plotting_data = dataset\n",
    "elif bin_by == 'week':\n",
    "    plotting_data = dataset.resample(time='1w').mean()\n",
    "    aggregated_by_str = 'Week'\n",
    "elif bin_by == 'month':\n",
    "    plotting_data = dataset.resample(time='1m').mean()\n",
    "    aggregated_by_str = 'Month'\n",
    "elif bin_by == 'weekofyear':\n",
    "    plotting_data = dataset.groupby('time.week').mean(dim=('time'))\n",
    "    aggregated_by_str = 'Week of Year'\n",
    "elif bin_by == 'monthofyear':\n",
    "    plotting_data = dataset.groupby('time.month').mean(dim=('time'))\n",
    "    aggregated_by_str = 'Month of Year'\n",
    "    \n",
    "params = dict(dataset=plotting_data, plot_descs={veg_proxy:{'none':[\n",
    "    {'box':{'boxprops':{'facecolor':'forestgreen'}}}]}})\n",
    "if plot_curve_fit:\n",
    "    params['plot_descs'][veg_proxy][curve_fit_target] = [{'gaussian_filter':{}}]\n",
    "    \n",
    "fig, curve_fit_plotting_data = \\\n",
    "    xarray_time_series_plot(**params, fig_params=dict(figsize=(8,4), dpi=150), \n",
    "                            max_times_per_plot=max_times_per_plot)\n",
    "plt.title('Box-and-Whisker Plot of {1} with a Curvefit of {0} {1}'\n",
    "          .format(curve_fit_target.capitalize(), veg_proxy))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span id=\"phenology_plot_2\">Plot the Vegetation Index Over Time for Each Year [&#9652;](#top)</span>\n",
    "Note that the curve fits here do not show where some times have no data (encoded as NaNs), as is shown in the box-and-whisker plot. Notably, the curve fits interpolate over times with missing data that are not the first or last time (e.g. January or December for monthly binned data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the binning approach for the vegetation index. Set the 'bin_by' parameter.\n",
    "# 'weekofyear'  = bin the data by week and years using a single year time axis\n",
    "# 'monthofyear' = bin the data by month and years using a single year time axis\n",
    "\n",
    "bin_by = 'monthofyear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "assert bin_by in ['weekofyear', 'monthofyear'], \\\n",
    "    \"The variable 'bin_by' can only have one of these values: \"\\\n",
    "    \"['weekofyear', 'monthofyear']\"\n",
    "\n",
    "years_with_data = []\n",
    "plot_descs = {}\n",
    "daysofyear_per_year = {}\n",
    "plotting_data_years = {}\n",
    "time_dim_name = None\n",
    "for year in range(start_date.year, end_date.year+1):\n",
    "    year_data = dataset.sel(time=slice('{}-01-01'.format(year), '{}-12-31'.format(year)))[veg_proxy]\n",
    "    if len(year_data['time']) == 0: # There is nothing to plot for this year.\n",
    "        print(\"Year {} has no data, so will not be plotted.\".format(year))\n",
    "        continue\n",
    "    years_with_data.append(year)\n",
    "    \n",
    "    spec_ind_dayofyear = year_data.groupby('time.dayofyear').mean()\n",
    "    daysofyear_per_year[year] = spec_ind_dayofyear[~spec_ind_dayofyear.isnull()].dayofyear\n",
    "    \n",
    "    aggregated_by_str = None\n",
    "    if bin_by == 'weekofyear':\n",
    "        plotting_data_year = year_data.groupby('time.week').mean(dim=('time'))\n",
    "        time_dim_name = 'week'\n",
    "    elif bin_by == 'monthofyear':\n",
    "        plotting_data_year = year_data.groupby('time.month').mean(dim=('time'))\n",
    "        time_dim_name = 'month'\n",
    "\n",
    "    plotting_data_years[year] = plotting_data_year\n",
    "    num_time_pts = len(plotting_data_year[time_dim_name])\n",
    "    \n",
    "    # Select the curve-fit type. \n",
    "    # See the documentation for `xarray_time_series_plot()` regarding the `plot_descs` parameter.\n",
    "    plot_descs[year] = {'mean':[{'gaussian_filter':{}}]}\n",
    "\n",
    "time_dim_name = 'week' if bin_by == 'weekofyear' else 'month' if bin_by == 'monthofyear' else 'time'\n",
    "\n",
    "num_times = 54 if bin_by == 'weekofyear' else 12\n",
    "time_coords_arr = np.arange(1, num_times+1) # In xarray, week and month indices start at 1.\n",
    "time_coords_da = xr.DataArray(time_coords_arr, coords={time_dim_name:time_coords_arr}, \n",
    "                              dims=[time_dim_name], name=time_dim_name)\n",
    "coords = dict(list(plotting_data_years.values())[0].coords)\n",
    "coords[time_dim_name] = time_coords_da \n",
    "plotting_data = xr.Dataset(plotting_data_years, coords=coords)\n",
    "params = dict(dataset=plotting_data, plot_descs=plot_descs)\n",
    "\n",
    "xarray_time_series_plot(**params, fig_params=dict(figsize=(8,4), dpi=150))\n",
    "plt.title('Line Plot of {0} for Each Year'.format(veg_proxy))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span id=\"export\">Export Curve Fits to a CSV File [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from calendar import monthrange\n",
    "\n",
    "# Convert the data to a `pandas.DataFrame`.\n",
    "curve_fit_df = curve_fit_plotting_data[(veg_proxy, 'median', 'gaussian_filter')]\\\n",
    "               .to_dataframe(name=veg_proxy)\n",
    "\n",
    "# xarray resample() uses the last day of each month when binning by month,\n",
    "# so we need to correct for that.\n",
    "first_date = pd.to_datetime(dataset.time.values[0])\n",
    "first_month = first_date.month\n",
    "first_year = first_date.year\n",
    "_, last_day_first_month = monthrange(first_year, first_month)\n",
    "first_month_last_date = pd.Timestamp(year=first_year, month=first_month, day=last_day_first_month)\n",
    "resample_time_offset = first_month_last_date - first_date\n",
    "curve_fit_df.index = curve_fit_df.index - resample_time_offset\n",
    "\n",
    "# Aggregate by day.\n",
    "curve_fit_df = curve_fit_df.resample('D').mean()\n",
    "\n",
    "# Convert the numpy.datetime64 times to strings.\n",
    "curve_fit_df.index.values[:] = np.datetime_as_string(curve_fit_df.index, unit='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the data to a CSV (if desired, remove two commented lines below)\n",
    "# Update the filename for different cases to avoid overwriting older files\n",
    "\n",
    "# csv_output_path = 'output/CSVs/DCAL_Vegetation_Phenology_sample.csv'\n",
    "# curve_fit_df.to_csv(csv_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span id=\"timesat\">Show [TIMESAT](http://web.nateko.lu.se/timesat/timesat.asp) Statistics [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TIMESAT_stats(dataarray, time_dim='time'):\n",
    "    \"\"\"\n",
    "    For a 1D array of values for a vegetation index - for which higher values tend to \n",
    "    indicate more vegetation - determine several statistics:\n",
    "    1. Beginning of Season (BOS): The time index of the beginning of the growing season.\n",
    "        (The downward inflection point before the maximum vegetation index value)\n",
    "    2. End of Season (EOS): The time index of the end of the growing season.\n",
    "        (The upward inflection point after the maximum vegetation index value)\n",
    "    3. Middle of Season (MOS): The time index of the maximum vegetation index value.\n",
    "    4. Length of Season (EOS-BOS): The time length of the season (index difference).\n",
    "    5. Base Value (BASE): The minimum vegetation index value.\n",
    "    6. Max Value (MAX): The maximum vegetation index value (the value at MOS).\n",
    "    7. Amplitude (AMP): The difference between BASE and MAX.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataarray: xarray.DataArray\n",
    "        The 1D array of non-NaN values to determine the statistics for.\n",
    "    time_dim: string\n",
    "        The name of the time dimension in `dataarray`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    stats: dict\n",
    "        A dictionary mapping statistic names to values.\n",
    "    \"\"\"\n",
    "    assert time_dim in dataarray.dims, \"The parameter `time_dim` is \\\"{}\\\", \" \\\n",
    "        \"but that dimension does not exist in the data.\".format(time_dim)\n",
    "    stats = {}\n",
    "    data_np_arr = dataarray.values\n",
    "    time_np_arr = _n64_datetime_to_scalar(dataarray[time_dim].values)\n",
    "    data_inds = np.arange(len(data_np_arr))\n",
    "    \n",
    "    # Obtain the first and second derivatives.\n",
    "    fst_deriv = np.gradient(data_np_arr, time_np_arr)\n",
    "    pos_fst_deriv = fst_deriv > 0\n",
    "    neg_fst_deriv = 0 > fst_deriv\n",
    "    snd_deriv = np.gradient(fst_deriv, time_np_arr)\n",
    "    pos_snd_deriv = snd_deriv > 0\n",
    "    neg_snd_deriv = 0 > snd_deriv\n",
    "    \n",
    "    # Determine MOS.\n",
    "    # MOS is the index of the highest value.\n",
    "    idxmos = np.argmax(data_np_arr)\n",
    "    stats['Middle of Season'] = idxmos\n",
    "    \n",
    "    data_inds_before_mos = data_inds[:idxmos]\n",
    "    data_inds_after_mos = data_inds[idxmos:]\n",
    "    \n",
    "    # Determine BOS.\n",
    "    # BOS is the last negative inflection point before the MOS.\n",
    "    # If that point does not exist, choose the first positive\n",
    "    # first derivative point before the MOS. If that point does\n",
    "    # not exist, the BOS is the MOS (there is no point before the MOS in this case).\n",
    "    snd_deriv_neg_infl = np.concatenate((np.array([False]), neg_snd_deriv[1:] & ~neg_snd_deriv[:-1]))\n",
    "    if snd_deriv_neg_infl[data_inds_before_mos].sum() > 0:\n",
    "        idxbos = data_inds_before_mos[len(data_inds_before_mos) - 1 - \n",
    "                                      np.argmax(snd_deriv_neg_infl[data_inds_before_mos][::-1])]\n",
    "    elif pos_fst_deriv[data_inds_before_mos].sum() > 0:\n",
    "        idxbos = np.argmax(pos_fst_deriv[data_inds_before_mos])\n",
    "    else:\n",
    "        idxbos = idxmos\n",
    "    stats['Beginning of Season'] = idxbos\n",
    "    \n",
    "    # Determine EOS.    \n",
    "    # EOS is the first positive inflection point after the MOS.\n",
    "    # If that point does not exist, choose the last negative\n",
    "    # first derivative point after the MOS. If that point does\n",
    "    # not exist, the EOS is the MOS (there is no point after the MOS in this case).\n",
    "    snd_deriv_pos_infl = np.concatenate((np.array([False]), pos_snd_deriv[1:] & ~pos_snd_deriv[:-1]))\n",
    "    if snd_deriv_pos_infl[data_inds_after_mos].sum() > 0:\n",
    "        idxeos = data_inds_after_mos[np.argmax(snd_deriv_pos_infl[data_inds_after_mos])]\n",
    "    elif neg_fst_deriv[data_inds_after_mos].sum() > 0:\n",
    "        idxeos = np.argmax(neg_fst_deriv[data_inds_after_mos])\n",
    "    else:\n",
    "        idxeos = idxmos\n",
    "    stats['End of Season'] = idxeos\n",
    "    \n",
    "    # Determine EOS-BOS.\n",
    "    stats['Length of Season'] = idxeos - idxbos\n",
    "    # Determine BASE.\n",
    "    stats['Base Value'] = data_np_arr.min()\n",
    "    # Determine MAX.\n",
    "    stats['Max Value'] = data_np_arr.max()\n",
    "    # Determine AMP.\n",
    "    stats['Amplitude'] = stats['Max Value'] - stats['Base Value']\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The minimum number of weeks or months with data for a year to have its stats calculated.\n",
    "# The aggregation used to obtain the plotting data determines which of these is used.\n",
    "\n",
    "min_weeks_per_year = 30\n",
    "min_months_per_year = 8\n",
    "\n",
    "# Select the binning approach for the vegetation index. \n",
    "# Set the 'bin_by' parameter.\n",
    "# 'week'        = bin the data by week\n",
    "# 'month'       = bin the data by month\n",
    "\n",
    "bin_by = 'month'\n",
    "\n",
    "assert bin_by in ['week', 'month'], \\\n",
    "    \"The variable 'bin_by' can only have one of these values: \"\\\n",
    "    \"['week', 'month']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from xarray.ufuncs import isnan as xr_nan\n",
    "from utils.data_cube_utilities.dc_time import _n64_datetime_to_scalar, _scalar_to_n64_datetime\n",
    "from utils_special.data_cube_utilities.curve_fitting import gaussian_filter_fit\n",
    "\n",
    "for year in years_with_data:\n",
    "    year_data = dataset.sel(time=slice('{}-01-01'.format(year), '{}-12-31'.format(year)))\\\n",
    "                    [veg_proxy].mean(['latitude', 'longitude'])\n",
    "    \n",
    "    if bin_by == 'week':\n",
    "        year_data = year_data.resample(time='1w').mean(dim='time')\n",
    "    elif bin_by == 'month':\n",
    "        year_data = year_data.resample(time='1m').mean(dim=('time'))\n",
    "    \n",
    "    non_nan_mask = ~xr_nan(year_data)\n",
    "    num_times = non_nan_mask.sum().values\n",
    "    year_data = year_data[non_nan_mask]\n",
    "    \n",
    "    if bin_by == 'week':\n",
    "        if num_times < min_weeks_per_year:\n",
    "            print(\"There are {} weeks with data for the year {}, but the \" \\\n",
    "                  \"minimum number of weeks is {}.\\n\".format(num_times, year, min_weeks_per_year))\n",
    "            continue\n",
    "    elif bin_by == 'month':\n",
    "        if num_times < min_months_per_year:\n",
    "            print(\"There are {} months with data for the year {}, but the \" \\\n",
    "                  \"minimum number of months is {}.\\n\".format(num_times, year, min_months_per_year))\n",
    "            continue\n",
    "    \n",
    "    # Fit a curve to the data to smooth it out for more reliable stats.\n",
    "    year_data_epochs = np.array([_n64_datetime_to_scalar(dt64) for dt64 in \n",
    "                                 year_data.time.values])\n",
    "    year_data_curve_fit_epochs, year_data_curve_fit_arr = \\\n",
    "        gaussian_filter_fit(year_data_epochs, year_data.values)\n",
    "    \n",
    "    year_data_curve_fit_times = \\\n",
    "        np.array([_scalar_to_n64_datetime(time) for time in year_data_curve_fit_epochs])\n",
    "    year_data = \\\n",
    "        xr.DataArray(year_data_curve_fit_arr, coords={'time': year_data_curve_fit_times}, dims='time',\n",
    "                     name=year_data.name, attrs=year_data.attrs)\n",
    "    \n",
    "    stats = TIMESAT_stats(year_data)\n",
    "    \n",
    "    # Map indices to days of the year.\n",
    "    time_arr = year_data.time.values\n",
    "    bos_date = pd.to_datetime(time_arr[stats['Beginning of Season']])\n",
    "    stats['Beginning of Season'] = bos_date\n",
    "    mos_date = pd.to_datetime(time_arr[stats['Middle of Season']])\n",
    "    stats['Middle of Season'] = mos_date\n",
    "    eos_date = pd.to_datetime(time_arr[stats['End of Season']])\n",
    "    stats['End of Season'] = eos_date\n",
    "    stats['Length of Season'] = np.abs(stats['End of Season'] - stats['Beginning of Season'])\n",
    "    \n",
    "    print(\"Year =\", bos_date.year)\n",
    "    print(\"Beginning of Season (BOS) day (MM/DD/YYYY) =\", f\"{bos_date.month}/{bos_date.day}/{bos_date.year}\")\n",
    "    print(\"Middle of Season (MOS) day (MM/DD/YYYY) =\", f\"{mos_date.month}/{mos_date.day}/{mos_date.year}\")\n",
    "    print(\"End of Season (EOS) day (MM/DD/YYYY) =\", f\"{eos_date.month}/{eos_date.day}/{eos_date.year}\")\n",
    "    print(\"Length of Season (abs(EOS-BOS)) in days =\", stats['Length of Season'].days)\n",
    "    print(\"Base Value (Min) =\", stats['Base Value'])\n",
    "    print(\"Max Value (Max) =\", stats['Max Value'])\n",
    "    print(\"Amplitude (Max-Min) =\", stats['Amplitude'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
